from .vector_store import load_vector_store
from .web_search import web_search
from streamlit import session_state as st_session
from langchain.schema import Document


def search_similar_documents(query: str, k: int = 3, score_threshold: float = 0.9) -> list[Document]:
    """
    PDF + DuckDuckGo ì›¹ ê²€ìƒ‰ ëª¨ë‘ ì‹¤í–‰ í›„ ê²°ê³¼ ë³‘í•©í•˜ì—¬ ë°˜í™˜
    """
    vectorstore = load_vector_store()
    results_with_score = vectorstore.similarity_search_with_score(query, k=k)

    print(f"\n[ğŸ“„ PDF ê²€ìƒ‰ ê²°ê³¼ (score í¬í•¨)] {len(results_with_score)}ê±´:")
    pdf_docs = []
    for i, (doc, score) in enumerate(results_with_score):
        print(f"  {i+1}. score={score:.2f} / {doc.page_content[:80].replace(chr(10), ' ')}...")
        if score >= score_threshold:
            pdf_docs.append(doc)

    # DuckDuckGo ì›¹ ê²€ìƒ‰ í•­ìƒ ìˆ˜í–‰
    print("[ğŸŒ DuckDuckGo ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...]")
    web_results = web_search(query)
    web_docs = []
    for i, item in enumerate(web_results):
        content = item.get("body", "") or item.get("title", "")
        source = item.get("href", "ì¶œì²˜ ì—†ìŒ")
        print(f"  [ì›¹ {i+1}] {content[:80].replace(chr(10), ' ')}... ({source})")
        web_docs.append(Document(page_content=content, metadata={"source": source}))

    # âœ… PDF + ì›¹ ê²°ê³¼ ë³‘í•©
    merged_docs = pdf_docs + web_docs

    print(f"[âœ… ìµœì¢… ë¬¸ì„œ ë³‘í•© ê²°ê³¼] PDF {len(pdf_docs)}ê±´ + ì›¹ {len(web_docs)}ê±´ â†’ ì´ {len(merged_docs)}ê±´")
    st_session["rag_documents"] = merged_docs

    return merged_docs
